{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e196b6dd",
   "metadata": {},
   "source": [
    "<h1>INTRODUCTION</h1>\n",
    "It is machine learning with multi-layered neural networks<br>\n",
    "It is a powerful subset of AI that learns from vast data through neural networks<br>\n",
    "Mimics human brain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee96af7",
   "metadata": {},
   "source": [
    "<h3>ML VS DL</h3>\n",
    "1. Feature Engineering (not required in DL)<br>\n",
    "2. Domain expertise required (in ML)<br>\n",
    "3. Complexity Struggle in ML (High dimensional data and complex patterns) <br>\n",
    "4. Performance Plateaus in ML (Accuracy saturation despite more data, eg. image recognition)<br>\n",
    "5. ML works better with structured data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a069fb6",
   "metadata": {},
   "source": [
    "<h3>DL ADVANTAGES</H3>\n",
    "1. Automatic Feature Extraction<br>\n",
    "2. High Modeling Complexity in capturing intricate relationships<br>\n",
    "3. Scalability (Handle large Datasets and leverage powerful hardware)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef963a2",
   "metadata": {},
   "source": [
    "<h3> USE CASES</h3>\n",
    "1.Image Recognition<br>\n",
    "2.NLP<br>\n",
    "3.Speech Recognition<br>\n",
    "4.Autonomous Driving<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a94dff",
   "metadata": {},
   "source": [
    "<h3>PERCEPTRON</h3>\n",
    "similar to biological neuron(also called as artificial neural network)<br>\n",
    "Structure:<br>\n",
    "1. input and weights<br>\n",
    "2. Activation Function <br>\n",
    "3. Output Transmission\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd56147",
   "metadata": {},
   "source": [
    "<h3>EQUATION</h3>\n",
    "z=w1.x1+w2.x2+w3.x3+b <br>\n",
    "w1,w2,w3 => weights<br>\n",
    "x1,x2,x3 => inputs <br>\n",
    "b        => bias (intercept)\n",
    "\n",
    "<h2>Activation Function</h2>\n",
    "Activation function helps in inducing the non-linearity in the model (else the model is same as linear regression which works only on linear data)<br>\n",
    "y=Activation(z)\n",
    "\n",
    "<H4>1.Sigmoid Function</H4>\n",
    "y=1/(1+e^(-z))\n",
    "<br>Disadvantage<br>\n",
    "1.vanishing gradient issues<br> \n",
    "2. not 0-centred \n",
    "<H4>2.Tanh Function</H4>\n",
    "scales values ranging from [-1,1]<br>\n",
    "better than sigmoid in producing efficient gradients<br>\n",
    "<h4>3.ReLU(Rectified Linear Unit)</h4>\n",
    "max(0,x)<br>\n",
    "Disadvantage:<br>\n",
    "prone to Dying ReLU: gives 0 for any input\n",
    "<h4>4.Leaky ReLU</h4>\n",
    "same as ReLU but for negative values f(x)=alpha*x, where alpha is a very small value\n",
    "<h4>5.Parametric ReLU</h4>\n",
    "addition to Leaky ReLU this helps in providing a better slope for negative values<br>\n",
    "Leaky and Parametric ReLU helps in solving the dying ReLU scenario"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434641d2",
   "metadata": {},
   "source": [
    "<br>\n",
    "<h4>WHEN TO USE WHICH FUNCTION</H4>\n",
    "Hidden Layers => ReLU or the Types of ReLU<br>\n",
    "Output Layers => <br>1.Regression - Linear\n",
    "                 <br>2.RNN + Binary Classification - Sigmoid\n",
    "                 <br>3.Multiclass Classification - Softmax<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7420d930",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "00147cd0",
   "metadata": {},
   "source": [
    "<h4>MLP(Multi Layered Perceptrons)</h4>\n",
    "inpput layer() -> hidden(intermediate) layers -> output layer(1)<br>\n",
    "Densely Connected Neurons <br>\n",
    "neurons in the same layer are not connected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17a3cc3",
   "metadata": {},
   "source": [
    "w<sup>1</sup><sub>11</sub> -> represents the weight from first layer neuron to the first hidden layer's first neuron\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2fd37e",
   "metadata": {},
   "source": [
    "z<sub>1</sub>=x<sub>1</sub>*w<sup>1</sup><sub>11</sub> + x<sub>2</sub>*w<sup>1</sup><sub>21</sub> + ... +b<sup>1</sup><sub>1</sub><br>\n",
    "this is the equation for the first neuron in the first hidden layer <br>\n",
    "we use matrix multiplication for multiplying wights and inputs<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b170b2",
   "metadata": {},
   "source": [
    "the result of one activation function is the input to the neuron of the next layer<br>\n",
    "eg.  O<sub>1</sub>=z<sub>1</sub>*w<sup>2</sup><sub>11</sub> + z<sub>2</sub>*w<sup>2</sup><sub>21</sub> + ... +b<sup>2</sup><sub>1</sub><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911b47e2",
   "metadata": {},
   "source": [
    "y<sub>cap</sub> => predicted value<br>\n",
    "y => actual value <br>\n",
    "error (loss function) |y - y<sub>cap</sub>|<br>\n",
    "there are different types of loss functions<br>\n",
    "choosing them wisely is a critical task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127bb32a",
   "metadata": {},
   "source": [
    "the updation of weights depending on the loss function is done from the output layer to the input layer<br>\n",
    "this process is known as back propogation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89475c0",
   "metadata": {},
   "source": [
    "<h3>GRADIENT DESCENT</h3>\n",
    "1.Stochastic GD<br>\n",
    "2.Mini Batch GD<br>\n",
    "It is an algorithm from the optimizer class which helps in updating the weights in an optimized way<br>\n",
    "this algorithm focusses on finding the global minima using differential equations<br>\n",
    "<h5></h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ef1596",
   "metadata": {},
   "source": [
    "epoch = forward+backward<br>\n",
    "epoch determines the number of times the model is being run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea967651",
   "metadata": {},
   "source": [
    "<h3>LOSS FUNCTION</h3>\n",
    "Cornerstone of machine learning model training, they guide in optimizing the models, gives an idea  us how deviated the predicted values are from the actual values, uses optimizer function to optimize the weights to reduce the loss<br>\n",
    "1. MSE<br>\n",
    "2. MAE<br>\n",
    "3. Huber Loss<br><br>\n",
    "For classification:<br>\n",
    "1. Binary Cross-Entropy<br>\n",
    "2. Categorical Cross-Entropy<br>\n",
    "3. Sparse Categorical Cross-Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4f372c",
   "metadata": {},
   "source": [
    "<h3>CHAIN RULE</h3>\n",
    "dL/dW = (dL/dy)*(dy/dz)*(dz/dp)*(dp/dz)*(dz/dO)*(dO/dW)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ab0bc5",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
